{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "acting-stream",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "declared-cylinder",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, alpha=0.1):\n",
    "        # initialize the list of weights matrices, then store the network architecture and learning rate\n",
    "        self.W = []\n",
    "        self.layers = layers\n",
    "        self.alpha = alpha\n",
    "        # start looping from the index of the first layer but\n",
    "        # stop before we reach the last two layers\n",
    "        for i in np.arange(0, len(layers)-2):\n",
    "            # randomly initialize a weight matrix connecting the number of nodes in each respective layer together adding  an extra node for the bias\n",
    "            w = np.random.randn(layers[i] + 1, layers[i + 1] + 1)\n",
    "            self.W.append(w / np.sqrt(layers[i]))\n",
    "        # the last two layers are a special case where the input connections need a bias term but the output does not\n",
    "        w = np.random.randn(layers[-2] + 1, layers[-1])\n",
    "        self.W.append( w / np.sqrt(layers[-2]))\n",
    "#         print(np.shape(self.W))\n",
    "#         print(\"[INFO] shape of W={}\".format(np.shape(self.W)))\n",
    "            \n",
    "    def __repr__(self):\n",
    "        # construct and return a string that represents the network architechture\n",
    "        return \"NeuralNetwork: {}\".format(\"_\".join(str(l) for l in self.layers))\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        # compute and return the sigmoid activation value for a given input value\n",
    "        return 1.0 / (1 + np.exp(-x))\n",
    "    \n",
    "    def sigmoid_deriv(self, x):\n",
    "        # compute the derivative of the sigmoid function ASSUMING that 'x' has already been passed through the 'sigmoid' function\n",
    "        return x * (1 - x )\n",
    "    \n",
    "    def fit(self, X, y, epochs=1000, displayUpdate=100):\n",
    "        # insert a column of 1's as the last entry in the feature matrix\n",
    "        # this little trick allows us to treat the bias as a trainable parameter within the weight matrix\n",
    "        X = np.c_[X, np.ones((X.shape[0]))]\n",
    "    \n",
    "        # loop over the desired number of epochs\n",
    "        for epoch in np.arange(0, epochs):\n",
    "#             print(\"-------------- epoch={}-----------\".format(epoch))\n",
    "            # loop over each individual data dpoint and train over network on it\n",
    "            for (x, target) in zip(X, y):\n",
    "                self.fit_partial(x, target)\n",
    "                \n",
    "            # check to see if we should display a training update\n",
    "            if epoch == 0 or (epoch + 1) % displayUpdate == 0:\n",
    "                loss = self = self.calculate_loss(X, y)\n",
    "                print(\"[INFO] epoch={}, loss={:.7f}\".format(epoch + 1, loss) )\n",
    "                \n",
    "    def fit_partial(self, x, y):\n",
    "        # construct our list of output activations for each layer as our data point flows through the network\n",
    "        # the first activation is a special case -- it's just the input feature vecor itself\n",
    "        A = [np.atleast_2d(x)]\n",
    "        \n",
    "        # FEEDFORWARD\n",
    "        # loop over the layers in the network\n",
    "        for layer in np.arange(0, len(self.W)):\n",
    "            # feedforward the activation at the current layer by taking the dot product between the activation and \n",
    "            # the weight matrix -- this is called the \"net input\" to the current layer\n",
    "            net = A[layer].dot(self.W[layer])\n",
    "            \n",
    "            # computing the \"net output\" is simply applying our nonlinear activation function to the net input\n",
    "            out = self.sigmoid(net)\n",
    "            \n",
    "            # once we have the net output, add it to our list of activations\n",
    "            A.append(out)\n",
    "            \n",
    "            # BACKPROPAGATION\n",
    "            # the first phase of backpropagation is to compute the difference between our prediction \n",
    "            #(the final ourput activation in the activations list) and the true target value\n",
    "            error = A[-1] - y\n",
    "            \n",
    "            # from here, we need to apply the chain rule and build our list of deltas 'D';\n",
    "            # the first entry in the deltas is simply the error of the output layer times the derivative of our activation function for the ourput value\n",
    "            D = [error * self.sigmoid_deriv(A[-1])]\n",
    "            \n",
    "            # once you understand the chain rule it becomes super easy to implement with a 'for' loop -- simply loop over the layers in reverse order \n",
    "            # (ignoring the last two since we already have taken them into account)\n",
    "            for layer in np.arange(len(A)-2, 0, -1):\n",
    "                delta = D[-1].dot(self.W[layer].T)\n",
    "                delta = delta * self.sigmoid_deriv(A[layer])\n",
    "                D.append(delta)\n",
    "           \n",
    "            # since we looped over our layers in reverse order we need to reverse the deltas\n",
    "            D = D[::-1]\n",
    "\n",
    "            # WEIGHT UPDATE PHASE\n",
    "            # loop over the layers\n",
    "            print(\"A:\", np.shape(A))\n",
    "            print(\"D:\", np.shape(D))\n",
    "#             print(\"W:\", np.shape(self.W))\n",
    "            for layer in np.arange(0, len(self.W)):\n",
    "                self.W[layer] += -self.alpha * A[layer].T.dot(D[layer])\n",
    "        \n",
    "    def predict(self, X, addBias=True):\n",
    "        p = np.atleast_2d(X)\n",
    "        \n",
    "        if addBias:\n",
    "            p = np.c_[p, np.ones((p.shape[0]))]\n",
    "            \n",
    "        for layer in np.arange(0, len(self.W)):\n",
    "            p = self.sigmoid(np.dot(p, self.W[layer]))\n",
    "        return p\n",
    "    \n",
    "    def calculate_loss(self, X, targets):\n",
    "        targets = np.atleast_2d(targets)\n",
    "        predictions = self.predict(X, addBias=False)\n",
    "        loss = 0.5 * np.sum((predictions - targets) ** 2)\n",
    "        return loss\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "concrete-surgeon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] shape of X=(4, 2)\n",
      "[INFO] shape of y=(4, 1)\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0,0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "print(\"[INFO] shape of X={}\".format(X.shape))\n",
    "print(\"[INFO] shape of y={}\".format(y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "instructional-ambassador",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork: 2_2_1\n",
      "A: (2, 1, 3)\n",
      "D: (1, 1, 3)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-162-f4393c2de961>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeuralNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-160-551f0f1dfb22>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, epochs, displayUpdate)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;31m# loop over each individual data dpoint and train over network on it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_partial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;31m# check to see if we should display a training update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-160-551f0f1dfb22>\u001b[0m in \u001b[0;36mfit_partial\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;31m#             print(\"W:\", np.shape(self.W))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maddBias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork([2, 2, 1], alpha=0.5)\n",
    "print(nn)\n",
    "nn.fit(X, y, epochs=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parallel-rental",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "third-shakespeare",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OpenCV",
   "language": "python",
   "name": "opencv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
